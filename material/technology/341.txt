Recent reports have called attention to troves of videos with profanity and violent themes on YouTube Kids, the video-sharing site's kid-friendly platform. Content for YouTube Kids is pulled from the main YouTube platform using machine learning and algorithms. But some videos, such as cartoons disguised as age appropriate, slip through the cracks. Videos flagged as inappropriate for kids will be slapped with an age-restriction warning on its main YouTube app. This will prevent them from being pulled onto the Kids app. Only those logged into YouTube who are over the age of 18 will be able to view it. Videos flagged with an age restriction (18 and older) are already banned from YouTube Kids. The app is geared toward children under 13. The Google-owned company announced it will roll out the new policy in the coming weeks. But it told CNN Tech the move is not in response to recent headlines but to improve upon the service. A Medium post made rounds this week from writer James Bridle, who detailed the content he's noticed on YouTube Kids for "some time." "Disturbing Peppa Pig videos, which [promote] extreme violence and fear, with Peppa eating her father or drinking bleach, are, it turns out very widespread," he wrote. "The architecture [YouTube and Google] built to extract the maximum revenue from online video is being hacked by persons unknown to abuse children," he added. Related: To combat revenge porn, Facebook wants some users to send their intimate photos Last week, YouTube's global head of family and learning content Malik Ducard told The New York Times the inappropriate videos were "the extreme needle in the haystack." One search for Peppa Pig on YouTube's main page turned up a video called "Cocaine pancakes" with nearly 1 million views. Multiple curse words appeared in the first 17 seconds alone -- but it is not yet flagged as age-restricted content. In the past 30 days, the company said that less than .005% of videos viewed in the Kids app were removed for being inappropriate. "Earlier this year, we updated our policies to make content featuring inappropriate use of family entertainment characters ineligible for monetization," said Juniper Downs, YouTube director of policy, in a statement. "The YouTube team is made up of parents who are committed to improving our apps and getting this right."