  Facebook and Amazon are putting cash into a project to lay a new submarine cable that will link Asia and the US. Once completed in 2020, the Jupiter cable will stretch for more than 14,000km and will be able to carry more than 60 terabits of data a second. The two firms have joined Jupiter as part of plans to build their own global networks and cut data transport costs. The cable is one of many in which the net's biggest firms, including Google and Microsoft, have recently invested. "There's a bit of a boom in terms of the internet content providers taking a leading role in the development of new submarine cable systems," said Alan Mauldin, research director at analyst firm Telegeography. The big net firms were involved in about 16 separate cabling ventures, he said, and used the capacity to handle the massive amounts of data their users generate. Some projects have already been completed but most are due to start carrying data sometime in the next few years. The net firms were most interested in joining projects that lay cables across the Pacific, the Atlantic and Asia, he said. They have acquired capacity on many other cables around the world as well. "They started years ago building their own networks because the scale that they need is bigger than a carrier can provide," he added. "Having a global backbone network is a big cost for them." By running their own networks, big net firms gain control over the system, keep costs low and get some redundancy in case of problems, he said. "Cables do break sometimes so you need multiple paths and alternatives," he said. Traditional telecoms firms were also investing in the cable-laying projects to help boost their own trans-ocean bandwidth, he said. Although the big net firms have colossal data needs, they could not use all the bandwidth available on the submarine cables. Mr Mauldin said the data needs of search firms, social networks and cloud providers were growing rapidly while some others, such as Netflix, did not have to splash out on large networks to handle their traffic. Netflix avoided high bandwidth costs by pushing all its content out to the edges of the network where it is stored ready for use. This worked, he said, because an episode of a TV show or a film would not change much once it was made and shipped out to media servers. By contrast, he said, the dynamic content seen on Facebook had to be updated constantly feeding a demand for more bandwidth.